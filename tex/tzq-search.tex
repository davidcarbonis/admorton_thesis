\chapter{Analysis Strategy and Event Selection}\label{chapter:tzq-search}
The next chapters of this thesis describe the search for the dilepton final state of a singly produced top quark in association with a Z boson (tZq) using the reconstructed proton-proton collision data at $\sqrt{13}$ collected by the CMS detector during 2016.

In contrast to previous analyses for tZq, which searched for the trilepton final state in which both the W and Z bosons decay leptonically~\cite{Sirunyan:2017kkr,Sirunyan:2017nbr}, the dilepton final state involves the W and Z bosons decaying hadronically and leptonically respectively.
This event topology presents the the main challenge for this analysis, as it is identical to the topology a large number of background processes which have cross sections many order of magnitude larger than the signal process.
Given that the signal region will inevitably be dominated by such backgrounds, it was vital that the search was designed to constrain and understand these backgrounds as much as possible.


As shown in Figure~\ref{fig:feyn_tZq}, at leading order tZq consists of a top quark, a recoil quark and a radiated Z boson.
Only

The trigger and event cleaning strategies common


The two largest irreducible 

Events with large multi-jet components, such as Z+jets and \ttbar, form the majority of the background contributions
Enriched control regions

Sources of non-prompt leptons, such as W+jets processes, 
Talk about what needs constraining - ie need for low mis-id for Z leptons, need to balance purity/efficiency for b-jets, and use of W decay products to reco top mass.


As 

\section{Trigger Strategy}\label{sec:triggerStrategy}
As the search for the tZq dilepton final state relies on the identification of the two leptons from the Z boson decay, the trigger strategy consists of selecting events from datasets identified by the presence of leptons.
Given that the signal process being searched for is dominated by background processes and will likely be limited by statistics, it is essential to reconstruct and select as many signal events as possible.
Ideally. the single and double lepton triggers with the lowest possible transverse momenta thresholds would be considered to ensure that the maximum possible statistics can be obtained over the largest possible phase space.
The high instantaneous luminosity at the start of the most luminous data taking periods however, required a number of the L1 triggers to be prescaled to prevent the trigger bandwidth constraints being exceeded.
As only the triggers considered for the ee channel were impacted by this, these triggers were chosen on the basis of largest amount data using unprescaled triggers with the lowest possible transverse momenta thresholds.

Table~\ref{tab:triggersDatasets} lists the triggers applied to data and MC events for each channel, including the e$mu$ final state which is considered for a \ttbar enriched control region discussed in Chapter~\ref{subsec:ttbarCR}.
%Whilst the Z boson can decay into a tau anti-tau pair, despite these leptons being simulated and reconstructed in the MC, they are not considered in the event selection for this analysis due to the difficulty simulating them due to their high mass.

\begin{table}[htbp]
\topcaption {
Triggers and datasets used for each decay channel.
}
\label{tab:triggersDatasets}
  \centering
   \resizebox{\textwidth}{!}{
   \begin{tabular}{ccc}
   \hline
   \textbf{Final State} & \textbf{Dataset} & \textbf{HLT Paths}  \\
   \hline
    ee & DoubleElectron & HLT\_Ele23\_Ele12\_CaloIdL\_TrackIdL\_IsoVL\_DZ \\
    & SingleElectron &  HLT\_Ele32\_eta2p1\_WPTight\_Gsf   \\
   \hline
    $\mu\mu$ & DoubleMuon  & HLT\_Mu17\_TrkIsoVVL\_(Tk)Mu8\_TrkIsoVVL\_DZ \\  
    & SingleMuon &  HLT\_Iso(Tk)Mu24  \\  
   \hline
   e$\mu$ & MuonEG &  HLT\_Mu12\_TrkIsoVVL\_Ele23\_CaloIdL\_TrackIdL\_IsoVL(\_DZ)   \\  
          &        &  HLT\_Mu8\_TrkIsoVVL\_Ele23\_CaloIdL\_TrackIdL\_IsoVL(\_DZ)  \\
          &        &  HLT\_Mu23\_TrkIsoVVL\_Ele12\_CaloIdL\_TrackIdL\_IsoVL(\_DZ) \\
    & SingleElectron &  HLT\_Ele32\_eta2p1\_WPTight\_Gsf   \\
    & SingleMuon &  HLT\_Iso(Tk)Mu24  \\  
   \hline
 \end{tabular}}
\end{table}

\section{Event Cleaning}\label{sec:metFilters}
Following the trigger requirements, a number of filters are applied in order to so that events with beam or detector anomalies which result in anomalous \MET are not considered for use in the analysis.

\begin{itemize}
\item \textbf{Primary Vertex Filter} - ensures that the primary vertex is well reconstructed by requiring it to be within $|z| \leq 24\cm$ of the interaction point and within $d_{0} < 2\cm$ of the beam line.
\item \textbf{Beam Halo Filter} - beam halos are machine induced particles (\eg beam-gas, beam-pipe interactions) which circulate with the beam at radii up to 5m. The filter removes events with calorimeter and muon chamber energy deposits consistent with either halo particles or particle showers caused by halos interacting with the collimator blocks that used to clean halos from the beam.
\item \textbf{HBHE Noise and Isolation Filters} - removes events where anomalous noise is present in the HCAL's hybrid photodiodes or readout boxes, which registers as large isolated energy deposits which would infer the presence of large \MET, by considering the channel multiplicities, pulse shape of the readout and the neighbouring activity in the calorimeters and tracker.
\item \textbf{ECAL Trigger Primitive Filter} - the L1 trigger primitive readout can be used to estimate the energy deposited in approximately 70\% of the channels which lack regular data links and are masked out for reconstruction. As trigger the primitives have a narrower energy acceptance range than the read-out, when the energy is near their saturation energy the measured energy is likely to be underestimated, resulting in high anomalous \MET. 
%\item \textbf{ECAL Endcap SC Filters} - NOT recommended for 2016
\item \textbf{Bad Charged Hadron Filter} - removes events where a muon is not defined as a PF muon due to its low quality, but makes its way into the PF MET calculation as a charged hadron candidate.
\item \textbf{Bad Muon Filter} - removes events where a muon is defined as a PF muon, but is still has too low a quality and large \pT to be considered.
\end{itemize}

\section{Physics Objects}\label{sec:eventSelection}
This section discusses the various selection criteria this analysis applies to the physics objects which have been identified and reconstructed using the particle flow algorithm described in Chapter~\ref{chapter:data-mc}.

\subsection{Lepton Selection}
All PF electrons and muons considered must pass a set of kinematic requirements and a set identification criteria defined by CMS.
The kinematic requirements are applied to ensure that the leptons lie within detector acceptance and that their transverse momenta lies in the region where the trigger's turn-on is well described.
The identification criteria have been designed to be efficient at selecting isolated prompt leptons and rejecting leptons which have been produced non-promptly from decays from within jets or taus or from incorrectly reconstructed tracks.

Different working points are used by identification criteria defined by CMS, where the lepton selection efficiency is traded off against the purity of the leptons selected.
For both lepton flavours the ``tightest'' working point is used to select high purity collections of leptons, with the ``loosest'' working point used to veto events with any additional leptons.

\subsubsection{Electrons}\label{subsubsec:electronSelection}
For any PF electron candidate to be considered it must meet the following kinematic requirements:

\begin{itemize}
\item the \pt of the leading and subleading electrons considered must be greater than the 35\GeVc and 15\GeVc respectively.
\item electrons must have $|\eta| \leq 2.50$ to ensure that the electrons are within the ECAL acceptance.
\item as accurate reconstruction cannot be undertaken in the transition region between the ECAL barrel and endcap, electrons with $1.4442 \leq \eta \leq 1.566$ are not considered.
\item the longitudinal impact parameter, $d_{z}$, of the electron must be less than 0.10\cm in the barrel and 0.20\cm in the endcap disks.
\item the transverse impact parameter, $d_{0}$, of the electron must be less than 0.05\cm in the barrel and 0.10\cm in the endcap disks.
\end{itemize}

The \emph{tight} and \emph{veto} working points (WPs) of the \emph{cut based} identification criteria which are approximately  70\% and 95\% efficient respectively, are used to select electrons and to veto any additional electrons.
The cut based identification uses a mixture of manually set and multivariate analysis (MVA) tuned variables for btoh the barrel and endcap disks.

The manually set variables are:
\begin{itemize}
\item \textbf{$N^{missing}_{inner hits}$} - as photons which subsequently convert do not leave hits in the innermost layers of the tracker, electrons are rejected if the expected number of missing hits is exceeded.
\item \textbf{a conversion veto} - is applied for all working points, where any electron which fails the electron conversion veto is rejected.
\end{itemize}

The MVA tuned variables include:
\begin{itemize}
\item \textbf{Full $5 \times 5 \sigma_{i\eta i\eta}$} - the shower shape variable, which describes the shape of the shower in $\eta$.
\item \textbf{$\Delta \eta_{seed}$ and $\Delta \phi_{seed}$} - the distances in $\eta$ and $\phi$ between the ECAL supercluster and where the track has been extrapolated to from the primary vertex.
\item \textbf{$\frac{h}{E}$} - the ratio of hadronic to electromagnetic energy deposited in the supercluster around the crystal with the largest energy deposit.
\item \textbf{$I^{rel}_{EA}$} the relative isolation of the electron with effective area pileup alleviation for a cone size of 0.3, which is described further in Chapter~\ref{subsubsec:relIso}.
\item \textbf{$1/E - 1/p$} - the difference in the inverse energy of the ECAL supercluster and inverse track momentum, which is used to describe the energy loss 
\end{itemize}

The cuts used for the tight and veto WPs for these variables are given in Table~\ref{tab:electronCuts}.

\begin{table}[htbp]
\topcaption {
The cuts used for the tight and veto working points of the cut based identification criteria for electrons for the barrel and endcap disks.
}
\label{tab:electronCuts}
  \centering
  \resizebox{\textwidth}{!}{
% This right-aligns numbers in column, but centers them under column title.
 \begin{tabular}{ccccc}
   \hline
   \textbf{Variable} & \multicolumn{2}{c}{\textbf{Tight WP}} & \multicolumn{2}{c}{\textbf{Veto WP}}   \\
    & Barrel & Endcap & Barrel & Endcap \\
    \hline   
    Full $5\times5 \sigma_{i\eta i\eta}$ & $< 0.00998$ & $< 0.0292$ & $< 0.0115$ & $< 0.037$ \\
    $\Delta \eta_{seed}$ & $<0.00308$ & $<0.00605$ & $<0.00749$ & $<0.00895$ \\
    $\Delta \phi_{seed}$ & $<0.0816$ & $<0.0394$ & $<0.228$ & $<0.213$ \\
    $\frac{h}{E}$ & $<0.0414$ & $<0.0641$ & $<0.356$ & $<0.211$	\\
    $I^{rel}_{EA}$ & $<0.0588$ & $<0.0571$ & $<0.175$ & $<0.159$ \\
    $1/E - 1/p$ & $<0.0129$ & $<0.0129$ & $<0.299$ & $<0.15$ \\
    $N^{missing}_{inner hits}$ & $\leq 1$ & $\leq 1$ & $\leq 2$ & $\leq 3$ \\
    pass conversion veto & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ \\
    \hline
 \end{tabular}}
\end{table}

\subsubsection{Muons}\label{subsubsec:muonSelection}
Similar to the electrons, PF muons are required to meet a set of kinematic requirements and identification and isolation criteria.

In the case of the kinematic requirements, PF muons candidates are required to:
\begin{itemize}
\item the \pt of the leading and subleading electrons considered must be greater than the 26\GeVc and 20\GeVc respectively.
\item muons must have $|\eta| \leq 2.50$ to ensure that the muon are within the acceptance of the muon systems.
\end{itemize}

The \emph{tight} and \emph{loose} identification and isolation criteria~\cite{Chatrchyan:2012xi} are used to select muons and veto any additional muons.

The tight muon criteria suppresses hadronic punch-through into the muon system and non-prompt muons, creating a high purity collection of particle flow muons.

These criteria are:
\begin{itemize}
\item muon a PF Muon and is also both a tracker and global muon.
\item $\chi^{2}/ndf$ of the global muon track fit is less than ten. 
\item at least one muon chamber is included in the global track fit.
\item that muon segments are found in at least two muon stations.
\item $d_{0} < 0.2\cm$ and $d_{z} < 0.5\cm$.
\item the muon must have at least one hit in the pixel detector.
\item hits must be present in at least six tracker layers in order to achieve a good \pT measurement.
\end{itemize}

The tight isolation cut applied to the resultant collection of tight muons is 95\% efficient, and rejects muons that have a relative isolation, with $\Delta\beta$ pileup corrections, greater than 0.15 for a cone size of 0.4.
This pileup correction for the relative isolation is described further in Chapter~\ref{subsubsec:relIso}.

Given that the loose cuts require the muon to be a particle flow muon and either a global muon or tracker muon, by definition all PF muons considered pass the loose identification cut.
The loose isolation cut is 98\% efficient and rejects muons with a relative isolation which is greater than 0.25.

\subsubsection{Lepton Isolation}\label{subsubsec:relIso}
A relative isolation variable $I^{rel}$ is used in order to:
\begin{itemize}
\item differentiate between leptons promptly produced at the primary vertex from those resulting from heavy jet or lepton decays.
\item to ensure that leptons are sufficiently separated from hadrons and photons to enable a precise momentum measurement of the lepton 
\end{itemize}

$I^{rel}$e is defined as the summed energy of all PF particles within a fixed radius cone of $\Delta R$ around the PF lepton, with the estimated neutral charged pileup contamination, $\rho$, removed, divided by the lepton \pT.

As only charged hadrons ($CH$) have associated tracks which can be used to determine if they are consistent with the primary vertex, the pileup contamination contribution from neutral hadrons ($NH$) and photons is typically estimated with one of two methods.

In the analysis presented, electrons use the $\rho$ * effective area ($rho * A_{\rm eff}$) technique using a $\Delta R$ of 0.3.
This method estimates the neutral pileup contributions by subtracting the median energy density per area of pileup contamination, $\rho$, which has been multiplied by the effective area of the electron, $A_{\rm eff}$, which is characterised as a function of the supercluster's $\eta$:

\begin{equation}
I^{rel}_{rho * A_{eff}} = \sum p_T(CH) + max (0.0, \sum E_{\rm T}(NH) + \sum E_{\rm T}(Photon) -rho*A_{\rm eff} )/p_T \\
\end{equation}\label{eq:rhoEffA}

The $\Delta\beta$ pileup mitigation method is used for muons using a $\Delta R$ of 0.4 in the analysis presented.
Using the fact that the ratio of neutral to charged hadron production in the hadronisation of pileup interactions is approximately 0.5, half of the transverse energy of charged hadrons from pileup is subtracted from the neutral hadron and photon transverse energies~\cite{Chatrchyan:2012vp}:

\begin{equation}
I^{rel}_{\Delta\beta} = \sum p_T(CH) + max (0.0, \sum E_{\rm T}(NH) + \sum E_{\rm T}(Photon) - 0.5 * \sum E_{\rm T}(PU))/p_T \\
\end{equation}\label{eq:deltaBeta}

\subsubsection{Z Boson Candidate Invariant Mass Requirements}
The presence of a Z boson in the final state requires that two leptons selected must be consistent with a Z boson decay.
Therefore, the leptons must have the same flavour and opposite charge and an invariant mass within 20\GeVcc of the known Z mass.
%This mass window was determined on the basis of 
%including sufficient signal events 

\subsection{Jet, b-tagging and W Boson Candidate Requirements}
\subsubsection{Jet Requirements}
Jets are considered from the PF jet collection which reconstructs jets using the \emph{anti-\kt} algorithm with R = 0.4 with charged hadrons originating from \PU vertices excluded from clustering.
Following identification, the jet energy corrections are applied as described in Chapter~\ref{subsubsec:JECs}.

Jets are considered in the analysis if they have a $\pT > 30\GeVc$, are within $|\eta| < 4.7$ and meet the \emph{loose} working point jet identification criteria developed by CMS.
In addition, selected leptons (electron or muon) which lie within a cone of $\Delta R = 0.4$ of a selected jet are not considered to be a prompt leptons and instead part of the jet in question.

The loose jet ID was designed to reject the majority of the fake tracks produced from detector and/or electronics noise whilst maintaining a high selection efficiency for real jets by requiring all jets to have part of their energy deposited in both the ECAL and HCAL and be composed of more than one particle.

\editComment{Necessary to spell this out? Or comment out ...}
The loose jet identification criteria are as follows:

for jets with $\eta \leq 2.70$ the loose ID criteria are:
\begin{itemize}
\item the fraction of the jet energy from both neutral electromagnetic particles in the ECAL and neutral hadronic particles in the HCAL is less than $0.99$.
\item at least two constituent particles are present.
\end{itemize}

with these applying in addition for $\eta \leq 2.40$:
\begin{itemize}
\item the fraction of the jet energy from charged electromagnetic particles in the ECAL is less than $0.99$ and greater than 0.0 for charged hadronic particles in the HCAL.
\item at least one charged particle is present.
\end{itemize}


for jets with $ 2.70 \leq \eta \leq 3.0$ the loose ID criteria are:
\begin{itemize}
\item the fraction of the jet energy from neutral electromagnetic particles in the ECAL is greater than than $0.01$ and less than $0.98$ for neutral hadronic particles in the HCAL.
\item at least three neutral particles are present.
\end{itemize}

and for jets with $\eta > 3.0$ the loose ID criteria are:
\begin{itemize}
\item the fraction of the jet energy in the ECAL that is from neutral electromagnetic particles is less than $0.90$.
\item at least eleven neutral particles are present.
\end{itemize}

\subsubsection{b-tagging}
The CSVv2 tagging algorithm described in Chapter~\ref{subsec:objReco-bJets} is used to tag jets, with a working point (WP) cut applied to the b-tag discriminator.
If the value of a jet's discriminator exceeds that of the Medium WP and has $|\eta| < 2.40$, the jet is considered to be a b-jet.
From the \emph{Loose}, \emph{Medium} and \emph{Tight} WPs defined by CMS~\cite{Sirunyan:2017ezt}, as given in Table~\ref{tab:bTagWPs} in Chapter~\ref{subsec:objReco-bJets}, the Medium WP was chosen as it provided the optimum performance in terms of providing as large statistics as possible for the signal process without too great a compromise on the purity of the selection.

\subsubsection{W Boson Candidate Invariant Mass Requirements}
In contrast to the previous tZq searches where the W boson decays into a lepton and its associated antineutrino, the W boson in the dilepton final state decays hadronically, allowing for the top quark to be fully reconstructed.
The W boson candidate is constructed by considering each possible pair of jets, with the pair with a dijet invariant mass closest to the known W boson mass of 80.4\GeVcc being chosen as the W candidate.
The leading b-jet however, is excluded from consideration as the hardest b-jet is assumed to be produced from the decay of the top quark. 

\subsection{\MET}\label{subsec:met}
Whilst the signal region does not explicitly cut on \MET, it is used in one of the Z+jets control regions, described in Chatper~\ref{subsec:zPlusJetsCR}, and in the boosted decision tree, discussed in Chapter~\ref{subsec:bdt}, in order to discriminate against backgrounds such as \ttbar which do feature significant amounts of \MET.

\section{Data and MC Simulation Samples}\label{sec:samples}
Out of the 37.8\fbinv of the proton-proton collision data at $\sqrt{13}$ collected by CMS during 2016, 35.8\fbinv was certified by the collaboration as ``good'' to be used for physics analysis.
The difference between the certified value and the total data recorded is the result of various factors such as the inavailability of a detector.
Due to the prescaling of the electron triggers during the start of the most luminous runs, as discussed in Chapter~\ref{sec:triggerStrategy}, the ee channel uses a reduced dataset of 35.6\fbinv were none of the triggers considered were prescaled.

Events in the double lepton and single lepton datasets from across these ``good'' data runs are considered where the double and single lepton triggers respectively have fired, using the strategy described in Chapter~\ref{sec:triggerStrategy}.

The MC samples used to signal and background processes that were considered are listed in Table~\ref{tab:mcList}, which includes information on the number of events generated, their cross sections and the order in perturbative accuracy in QCD to which the generators calculated the processes.


To determine the impact of a number of theoretical uncertainties for a number of processes, a several dedicated samples, listed in Table~\ref{tab:theorySampleList}, were used.
These systematic uncertainties are discussed further in Chapter~\ref{sec:theorySysts}.

For all the MC samples considered, the hadronisation of all the MC samples considered was undertaken using PYTHIA 8.
The NNPDF3.0 family of PDF sets was used as input for the generators of the MC samples, where the corresponding PDF sets were used depending on whether the sample was produced at LO or NLO and used either the four or five flavour scheme.

\begin{table}[htbp]
\topcaption {
The MC processes and their associated total number of events, cross sections and generators (and order in perturbative QCD accuracy they are calculated to), considered for the search for tZq in the dilepton final state. Both generators considered for the Z+jet background are also listed below.
}
\label{tab:mcList}
  \centering
  \resizebox{\textwidth}{!}{
% This right-aligns numbers in column, but centers them under column title.
 \begin{tabular}{cccc}
   \hline
   \textbf{MC process} & \textbf{Events} & \textbf{Cross section (pb)} & \textbf{Generator (Order)}   \\
   \hline
   tZq  & 14.5M & 0.0758  & aMC@NLO (NLO) \\
   \hline
   tHq  & 3.5M & 0.07462  & Madgraph (LO) \\
   \hline
   tWZ/tWll  & 50K & 0.01104  & Madgraph (LO) \\
   \hline
   t tW-channel & 7M & 35.85 & POWHEG (NLO) \\
   $\overline{\text{t}}$ tW-channel & 6.9M & 35.85 & POWHEG (NLO) \\
   \hline
   t s-channel & 2.9M & 10.32 & aMC@NLO (NLO) \\
   \hline
   t t-channel & 67.2M & 136.02 & POWHEG (NLO) \\
   $\overline{\text{t}}$ t-channel & 38.8M & 80.95 & POWHEG (NLO) \\
   \hline
   \ttbar & 77.1M & 831.76 & POWHEG (NLO) \\
   \hline
   \ttbarZ $\rightarrow$ ll$\nu\nu$ & 13.9M & 0.2529   & aMC@NLO (NLO) \\
   \ttbarZ $\rightarrow$ qq & 749K & 0.5297   & aMC@NLO (NLO) \\
   \hline
   \ttbarW $\rightarrow$ l$\nu$ & 5.3M & 0.2001   & aMC@NLO (NLO) \\
   \ttbarW $\rightarrow$ qq & 833K & 0.405  & aMC@NLO (NLO) \\
   \hline
   \ttbarH $\rightarrow$ bb & 3.8M & 0.2942 & POWHEG (NLO) \\
           $\rightarrow$ non bb & 4.0M & 0.2123 & POWHEG (NLO) \\
   \hline
   W+jets & 24.1M & 61526.7 & aMC@NLO (NLO) \\
   \hline
   Z+jets ($m_{Z} \geq 50\GeVcc $ & 146M & 5765.4 & Madgraph (LO) \\
   Z+jets ($10 \GeVcc \leq m_{Z} < 50\GeVcc$ & 35.3M & 18610.0 & Madgraph (LO) \\
   \hline
   Z+jets ($m_{Z} \geq 50\GeVcc $ & 151M & 5765.4 & aMC@NLO (NLO) \\
   Z+jets ($10 \GeVcc \leq m_{Z} < 50\GeVcc$ & 106M & 18610.0 & aMC@NLO (NLO) \\
   \hline
   WW $\rightarrow$ l$\nu$qq & 9.0M & 49.997  & POWHEG (NLO) \\
      $\rightarrow$ ll$nu\nu$ & 2.0M & 12.178 & POWHEG (NLO) \\
   \hline
   WZ $\rightarrow$ l$\nu$qq & 24.2M & 10.73 & aMC@NLO (NLO) \\
      $\rightarrow$ llqq & 26.5M & 5.606 & aMC@NLO (NLO) \\
      $\rightarrow$ lll$\nu$ 1.9M & 5.26 & aMC@NLO (NLO) \\
   \hline
   ZZ $\rightarrow$ ll$\nu\nu$ & 8.8M & 0.5644 & POWHEG (NLO) \\
      $\rightarrow$ llqq & 15.3M & 3.222 & aMC@NLO (NLO) \\
      $\rightarrow$ llll & 10.7M & 1.204 & aMC@NLO (NLO) \\
   \hline
   WWW & 240K & 0.2086 & aMC@NLO (NLO) \\
   \hline
   WWZ & 250K & 0.1651 & aMC@NLO (NLO) \\
   \hline
   WZZ & 247K & 0.05565 & aMC@NLO (NLO) \\
   \hline
   ZZZ & 249K & 0.01398 & aMC@NLO (NLO) \\
   \hline
   
 \end{tabular}}
\end{table}

\begin{table}[htbp]
\topcaption {
The dedicated MC samples used to determine the impact of theoretical uncertainties, including the associated total number of events, cross sections and generators (and order in perturbative QCD accuracy they are calculated to), considered for the search for tZq in the dilepton final state.
}
\label{tab:theorySampleList}
  \centering
 \resizebox{\textwidth}{!}{
 \begin{tabular}{cccc}
   \hline
   \textbf{MC process} & \textbf{Events} & \textbf{Cross section (pb)} & \textbf{Generator (Order)}   \\
   \hline
   tZq scale up & 6.9M & 0.0758  & aMC@NLO (NLO) \\
   tZq scale down & 7.0M & 0.0758  & aMC@NLO (NLO) \\
   \hline
   t tW-channel scale up & 998K & 35.85 & POWHEG (NLO) \\
   t tW-channel scale down & 994K & 35.85 & POWHEG (NLO) \\
   $\overline{\text{t}}$ tW-channel scale down & 1.0M & 35.85 & POWHEG (NLO) \\
   $\overline{\text{t}}$ tW-channel scale down & 999K & 35.85 & POWHEG (NLO) \\
   \hline
   t t-channel scale up & 5.7M & 136.02 & POWHEG (NLO) \\
   t t-channel scale down & 5.9M & 136.02 & POWHEG (NLO) \\
   t t-channel matching up & 6.0M & 136.02 & POWHEG (NLO) \\
   t t-channel matching down & 6.0M & 136.02 & POWHEG (NLO) \\
   $\overline{\text{t}}$ t-channel scale up & 4.0M & 80.95 & POWHEG (NLO) \\
   $\overline{\text{t}}$ t-channel scale down & 3.9M & 80.95 & POWHEG (NLO) \\
   $\overline{\text{t}}$ t-channel matching up & 4.0M & 80.95 & POWHEG (NLO) \\
   $\overline{\text{t}}$ t-channel matching down & 4.0M & 80.95 & POWHEG (NLO) \\
   \hline
   \ttbar ISR up & 156.5M & 831.76 & POWHEG (NLO) \\
   \ttbar ISR down & 149.8M & 831.76 & POWHEG (NLO) \\
   \ttbar FSR up & 152.6M & 831.76 & POWHEG (NLO) \\
   \ttbar FSR down & 156.0M & 831.76 & POWHEG (NLO) \\
   \ttbar matching up & 58.9M & 831.76 & POWHEG (NLO) \\
   \ttbar matching down & 58.2M & 831.76 & POWHEG (NLO) \\
   \hline   
 \end{tabular}}
\end{table}

\section{Simulation Corrections}\label{sec:simCorrections}
Simulation is unable to fully recreate all the effects observed in data, either because certain parameters are not precisely known or cannot be be calculated.
To account for these discrepancies, corrective scale factors are used to reweight MC on a per event basis.
Such scale factors are usually derived as a function of \pt and $\eta$ so as to account for the variation of the detector response in both.
These corrections are used to correct simulation for lepton identification, isolation and reconstruction efficiencies, b-tagging efficiencies, the poor modelling of pileup in simulation, and the detector resolutions observed in data.

\editComment{add SF distributions in appendix? Noticed others do this...}

\subsection{Miscalibrated Tracker APV}\label{subsec:hipEffect}
During the first half of data taking in 2016 the silicon strip detector suffered from instantaneous luminosity dependent  hit finding inefficiencies, particularly in high occupancy regions, due to saturation in the pre-amplifier in the front end electronics~\cite{Fiori:2016ebh}.
This issue was resolved by changing the configuration of the electronics.
While the affected part of the dataset has been reprocessed to mitigate the impact on the quality of the data taken, there is still a negative impact on the detector efficiency for objects that rely upon highly efficient tracking data.
This is accounted for by the weighting of events appropriately when the scale factors are produced, either centrally by CMS or those derived for the analysis (\ie the trigger scale factors), so that a single scale factor is applied to a simulated event.

%In most cases, centrally produced scale factors are derived for the whole of the 2016 dataset to account for this, but 
%for muons they were provided for both the affected and unaffected parts of the dataset separately.
%Consequently, the muon scale factors were weighted according to the luminosity they corresponded to before their application to simulation.
%Similarly, the muon trigger scale factors that were derived were also produced 

\subsection{Lepton Efficiency}\label{subsec:leptonRecoSFs}
The identification, isolation and reconstruction efficiencies of leptons are calculated using measurements of $Z \rightarrow l^{+} l ^{-}$ events with the \emph{tag-and-probe} method~\cite{CMS:2008rxa}.
Using events within a dilepton invariant mass window to ensure a high purity, from this large statistics lepton sample, the method ``tags'' and ``probes'' the leptons where one has passed a tight and the other a loose selection criteria.
For source of each efficiency and lepton flavour, the efficiency is given as the fraction of events where the probe leptons passed the relevant selection criteria.
This methodology is used to create corrective scale factors for each component and these are multiplicatively applied to each leptons' event weight as functions of their \pt, $\eta$, and flavour.

The trigger efficiency of electrons and muons is calculated using a method which considers events that pass the lepton selection criteria in the signal region~\ref{sec:signalRegion} and which are selected by triggers which are weakly correlated (also known as cross triggers) with the triggers used in the analysis~\cite{Khachatryan:2016kzg}.
From this collection of events, the number of events that pass and fail the analysis triggers are counted to produce the trigger efficiency:

\begin{equation}
\epsilon_{trigger} = \frac{N_{X triggers + lepton triggers}}{N_{X triggers}} \\
\end{equation}

where $N_{X trigger}$ is the number of events which have passed the lepton selection criteria and the cross triggers, and $N_{X triggers + lepton triggers}$ is $N_{X trigger}$ and the number of events which have also passed the lepton triggers.

As the triggers requirements are applied to both simulated and data events, a scale factor of the ratio of the trigger efficiency in data and in simulation is applied to the event weight in simulation.

%For the scale factors derived for the ee and e$\mu$ channels, a constant scale factor was found to be sufficient to account for the differences between data and siulation.
%In the case of the $\mu\mu$ channel however, the scale factor was produced as a function of both \pt and $\eta$ due to the trigger turn-on curve in data being impacted by the miscalibrated tracker APV (as discussed in Chapter~\ref{subsec:hipEffect}.

\subsection{Lepton Energy Corrections}\label{subsec:leptonEnergyCorrections}
\subsubsection{Electron Regression and Energy Scale and Smearing Corrections}
Two types of energy corrections which have been produced by the CMS EGM POG are applied to electrons and photons, energy regression and energy scale and smearing corrections.
These corrections are applied to both MC simulation and data and are used to improve the electron resolution obtained and to resolve the observed discrepancies between them.

Using simulation for tuning, energy regression obtains the best possible energy resolution by using the detector information to correct the reconstructed object energy.
The disagreement between data and MC is resolved by scaling the data energy to the MC energy scale and smearing the MC so that it has the same energy resolution as data. 

These corrections are pre-applied onto the PF electron collections used.

\subsubsection{Rochester Corrections}
The muon momentum scale and resolution correction methods developed by the University of Rochester~\cite{rochester}, known as \emph{Rochester Corrections}, are used to remove any muon momentum bias from any detector misalignment, reconstruction or uncertainties in the magnetic field for both MC and data.
These corrections are derived with high \pt ($> 20\GeVc$) muons from Z $ \rightarrow \mu\mu$ decays using a two step method, where the muons are binned in charge, $\eta$ and $\phi$.
The first step requires the mean inverse transverse momenta of the muons reconstructed from data and simulation to be the same as the corresponding values from a perfectly aligned detector.
These corrections are tuned in the second step by using the $M_{\mu^{+1}\mu^{-1}}$ peak for a perfectly aligned detector to calibrate the corrections.
This removes any sensitivity to detector efficiencies or physics modelling.

The Rochester Corrections are applied to each muon an event weight that is a function of the muon's charge, \pt, $\eta$ and $phi$.

\subsection{Jet Energy Corrections}\label{subsec:jesjer}
As described in Chapter~\ref{subsubsec:JECs}, the JECs are applied to account for the non-uniform response in \pT and $eta$ of the detector by comparing the differences between the generator level and detector level responses.

In addition to these corrections, as the Jet Energy Resolution (JER) observed in data is approximately 10\% poorer than that in observed simulation, the 4-vectors of simulated jets are smeared as functions of generator level and reconstructed \pt and $\eta$ to account for this~\cite{Khachatryan:2016kdb}.

\subsection{b-tagging Efficiency}\label{subsec:btagEff}
The B-Tag and Vertexing (BTV) Physics Object Group measures the b-tagging efficiency and misidentification rates for b and light flavoured jets in data and MC simulation (multijet and \ttbar) of the algorithms which they support~\cite{Sirunyan:2017ezt}.
From these measurements b-tagging efficiency scale factors are produced and provided for analysts to apply to simulated events to correct differences observed between data and simulation.
These scale factors, as functions of the jet flavour, \pT and $\eta$, to alter the weight of the selected MC events.
This methodology was chosen as it involves only changing the weight of the selected MC events which, unlike other methods, avoids events migrating into different b-tag multiplicity bins and having events with potentially undefined variables such as the top mass.

\subsection{\PU Modelling}\label{subsec:puSF}
It is challenging to model variations in the number of \PU interactions that result from the changing LHC conditions.
Therefore MC events are reweighted as a function of the number of primary vertices so that the PU interactions simulated are resembles what is observed in data.

The \PU SF is determined as a function of the number of primary vertices, $n_{PV}$, present by comparing $n_{PV}$ in minimum bias data over the running period considered to $n_{PV}$ for simulated events.

\subsection{Top quark \pt}
A scale factor is applied to \ttbar MC as a function of the top's and anti-top's generator level transverse momenta to account for the \pt spectra of top quarks in data being significantly softer than that predicted by LO and NLO precision MC simulation~\cite{Khachatryan:2015oqa}.

\section{Signal Region}\label{sec:signalRegion}
The signal enriched region event selection was designed select signal events with a high efficiency whilst rejecting as much of the background process as possible.
To ensure that the leptons selected are promptly produced and are of a high purity, exactly two leptons of the same flavour which pass the tight identification and isolation criteria, wi


As the tZq dilepton final state is distinguished by the presence of exactly two leptons from the Z boson decay, being able to identify these isolated prompt leptons with a high efficiency and a low misidentification rate is essential.

To ensure a 
Whilst additional jets could be produced by further gluon splitting, in practice 

No \MET cut is applied in the signal region event selection despite there being no \MET directly produced by the signal process.
This decision was taken as \MET was anticipated to be a useful variable the multivariate analysis (discussed in Chapter~\ref{sec:mvas}) that could be used to discriminate against background processes such as \ttbar.

\section{Control Regions}\label{sec:controlRegions}
In any high energy particle physics analysis, accurate modelling of the background processes is essential in order to be able to make a measurement.
As the main challenge in searching for the dilepton final state of tZq is that the signal region is dominated by background processes, it particularly important to ensure that the background processes are accurately modelled in simulation.
In order to confirm whether or not simulation adequately describes the data and thus if data-driven estimations are required instead, background enriched control regions which are topologically similar and orthogonal to the signal region are used for the two largest background processes are defined.
The same trigger, event cleaning, number of oppositely charged leptons, number of jets required and W and Z boson mass selection criteria are applied to the control regions so that they occupy a topologically similar phase space to the signal region.
Where required, these control regions are extrapolated to provide a data-driven estimation of the background in the signal region as discussed in Chapter~\ref{sec:dataDrivenBackground}.

\subsection{Z+jets Background Control Regions}\label{subsec:zPlusJetsCR}
Despite the majority of such events being rejected by the signal region criteria, given the large cross section for Z+jets, the remaining events from this process dominate the signal region to form the single largest background.
Given the size of this background and the difficulties in accurately modelling higher order contributions from QCD processes, it is essential to ensure that both the normalisation and modelling of the jets for this background are well described.

Initially a high statistics Z+jets enriched control region was defined by requiring that none of the jets present are b-tagged, in contrast to the one to two required for the signal region, as the majority of the jets 
Given the large cross section and that the vast majority of the jets produced by the Z+jets process are light jets, whilst the top quarks in \ttbar, the second largest background, predominately decay into b quarks, this produces a high purity region with large statistics.

Despite the good description of the jet multiplicity and jet \pT shapes in this control region, discussed in depth in Chapter~\ref{subsec:zPlusJetsEstimation}, as the kinematics of b-tagged jets differs from light jets, the topology may 

Therefore an alternative Z+jets enriched control region was defined by requiring same b-jet selection (one to two b-jets) as the signal region and a \MET cut of 50\GeV is applied to suppress \ttbar where significant quantities of \MET are expected from the leptonically decaying W bosons.

The application of both of these control regions is discussed in Chapter~\ref{sec:dataDrivenBackground}.

\subsection{\ttbar Background Control Region}\label{subsec:ttbarCR}
\ttbar events form the second largest background process, where events with two leptons produced from W bosons decaying having an invariant mass which is compatible with the Z boson mass window in the being selected in the signal region.
The selection criteria for the \ttbar control region differs from the signal region definition defined in Chapter~\ref{sec:signalRegion}, by requiring that the two oppositely leptons selected to have different flavours (\ie one is an electron and the other a muon).
%The electron and muon \pt thresholds are modified from those given in Chapters~\ref{subsubsec:electronSelection} and~\ref{subsubsec:muonSelection} as the leading lepton \pt thresholds of the HLT paths considered for the e$\mu$ final state, as given in Chapter~\ref{sec:triggerStrategy}, differ from those for the same flavour final states.

As the branching ratio for a W boson (produced by the top quark and anti-top quark decays) to decay into either an electron or muon is the same, this produces a \ttbar enriched background control region which is topologically similar to the \ttbar contributions in the signal region. 

\begin{table}[htbp]
\topcaption {
The event yields after the selection criteria have been applied for the \ttbar control region.
}
\label{tab:ttbarCR}
  \centering
% This right-aligns numbers in column, but centers them under column title.
 \begin{tabular}{cc}
   \hline
   \textbf{MC process} & \textbf{$e\mu$}  \\
   \hline
   tZq & 1.709\\
   \ttbar & 11778.461 \\
   Z+jets & 80.9921\\
   tW & 488.632\\
   Other & 166.200\\
   \hline
   Data & 12509.0 \\
   Total MC & 12515.995 \\
   \hline
 \end{tabular}
\end{table}

%\section{Blinding}\label{sec:blinding}
