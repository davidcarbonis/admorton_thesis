\chapter{Background Estimation}\label{chapter:bkg}
Despite 

\section{Data-driven Background Esimation}\label{sec:dataDrivenBackground}

\subsection{Non-Prompt Leptons}\label{sec:NPLs}
Backgrounds which involve decays into lepton + jets and where at least one jet is incorrectly reconstructed as a lepton (predominately electrons) or a lepton from the decay of heavy quarks (predominately muons), which pass the lepton selection and isolation criteria, are estimated with data.

The estimation of this background uses the same methodology as when performing top quark pair production~\cite{CMS:2016syx} and same-sign SUSY searches~\cite{CMS:2015vqc}.
The vast majority of the same-sign event yields found are the result of non-lepton and charge misidentified leptons, with some contribution from prompt leptons.
As these backgrounds are independent of the charge of the lepton pairs, it is expected that the nominal (opposite-sign) sample would have a similar contribution \cite{CMS:2015vqc}.

To estimate this contribution of opposite-sign non-prompt leptons in data, the same-sign event yields with the expected prompt-lepton contribution subtracted, is multiplied by a ratio of opposite-sign over same-sign non-prompt lepton events taken from MC.

The method requires that the same-sign control region established uses the same selection criteria as the nominal signal region, albeit with same-sign lepton pairs instead of opposite-sign ones.
This control region is dominated by non-lepton lepton events, but also contains contributions from prompt lepton events, charge misidentification and real same-sign pairs.

This data driven estimate is obtained using the following equation:

\begin{equation}
 N_{data}^{OS non-prompt} = (N_{data}^{SS} - N^{SS}_{real + mis-ID}).\frac{N_{MC}^{OS non-prompt}}{N_{MC}^{SS non-prompt}}
\end{equation}

where $N_{data}^{SS}$ is the total number of same sign events observed in data, $N^{SS}_{real + mis-ID}$ is the expected number of real same-sign events and events with charge misidentification and $N_{MC}^{OS non-prompt}$ and $N_{MC}^{SS non-prompt}$ the number of opposite-sign and same-sign non-prompt leptons observed in MC used to appropriately scale the estimate.

This ratio of MC opposite-sign over same-sign events is referred to as R, and is calculated using generator level information from reconstructed objects which have matched to a generator level particle. R is calculated from the W + jets, \ttZ and \ttW leptonic decaying, and single top MC samples with sufficient statistics given that these processes are expected to be the predominant source of non-prompt leptons for this analysis. 

\begin{table}[!htbp]
\centering
\begin{tabular}{| l |  c |  c |  c |  c |  c |}
\hline
Source &  $ee$ & $\mu\mu$ & Combined \\ 
\hline
\ttbar (SS): & a$\pm$b &  c $\pm$d & e$\pm$f    \\
Z + jets (SS): & a$\pm$b &  c$\pm$d & e$\pm$r    \\
Single Top (SS): & a$\pm$b & c$\pm$d & e$\pm$r    \\
VV (SS): & a$\pm$b & c$\pm$d & e$\pm$f    \\
ttV (SS): & a$\pm$b &  c$\pm$d & e$\pm$f    \\ 
\hline
Total background (SS): & a$\pm$b & c$\pm$d & e$\pm$f   \\ 
Data: & a$\pm$b & c$\pm$d & e$\pm$f    \\ 
\hline
SS data (bkg): & a$\pm$b & c$\pm$d & e$\pm$f \\
\hline
Non-prompt (SS): & a$\pm$b & c$\pm$d & e$\pm$f \\
Non-prompt (OS): & a$\pm$b & c$\pm$d & e$\pm$f \\
R (OS/SS): & a$\pm$b & c$\pm$d & e$\pm$f \\
\hline
Non-prompt estimation: & a$\pm$b & c$\pm$d & e$\pm$f \\
\hline
\end{tabular}
\caption{Non-prompt lepton estimation following all selection cuts}
\label{tab:fakeLeptonYields}
\end{table}

\subsection{Z+jets background}\label{subsec:zPlusJetsEstimation}
Madgraph - normalises well but poor jet multiplicity
aMC@NLO - bad normalisation, but good higher jet multiplicity description

\subsection{\ttbar background}\label{subsec:ttbarEstimation}
The \ttbar enriched control region defined in Chapter~\ref{subsec:ttbarCR} was designed to provide an orthogonal region which was topologically similar to the signal region in order to validate:
 whether or not the simulated \ttbar sample used accurately modelled the \ttbar process
 and if not, to be used to derive a data-driven estimate for \ttbar.



\section{Multivariate Analysis Techniques}\label{sec:mvas}
Multivariate Analysis (MVA) techniques are commonly used to further discriminate between signal and background processes given the difficulty in identifying rare or background dominated processes through the sole use of individual cuts.

Therefore, given the small cross section and topology of the dilepton final state of tZq, a MVA method was used to enhance the separation between the signal and background following the application of the selection cuts described in Chapter~\ref{chapter:tzq-search}.
The \emph{Boosted Decision Tree}~(BDT) MVA technique was used as it a widely used and supported technique which those undertaking this analysis were familiar with.

\subsection{Boosted Decision Trees}\label{subsec:bdt}
A decision tree in its simplest form is a series of sequential binary decisions (nodes) on a single variable at a time in order classify an event as signal or background.

\emph{Boosting} extends the concept of a decision tree from a single tree to a forest of trees with the aim of both stabilising the decision trees' response and enhancing their performance.
This involves training multiple trees in succession on the same training sample which has been reweighted based on the past trees' performance.
At the end of the process all the trees are combined into a single classifier which is given by the weighted average of the trees, thus creating a strong learner out of an ensemble of weak learners.

\emph{Bagging} is a similar concept to boosting, involving each tree being trained on a random subset of the training sample, where every element has an equal probability of being sampled.
As 

The \emph{Adaptive Boost} (AdaBoost) algorithm is a popular 

This exponential loss approach however, is its degraded performance in noisier environments as it is not robust in 	


\emph{Gradient Boosting} on 
The \emph{XGBoost} algorithm is similar to the Gradient Boost algorithm, 

The number of nodes or \emph{depth} of the tree considered
As each node's criteria are dependent on those which have preceded it, the decision tree is capable of separating signal 

\emph{Overtraining}
A training sample, consisting of a subset of the data to be classified, is used as the input 


Following the evaluation of a number of different versions of various BDT algorithms, the XGBoost algorithm as implemented in the ``XGBoost Library'' was determined to provide the optimal performance for the search presented~\cite{xgboost}.



BDT features and hyperparameters chosen separately for ee and mumu channels
Features chosen using recursive feature elimination
Hyperparameters are selected by using a Gaussian process to optimise the classifierâ€™s performance
hyperparameters = learning rate, n-estimators, max tree depth
feature = input variable

\subsubsection{BDT input variables}

From the 

\begin{table}[htbp]
\topcaption { The name and descriptions of the variables chosen by recursive feature elimination to be used as input to the BDT to discriminate between potential tZq signal events and the dominant.
}
\label{tab:bdtVariables}
  \centering
% This increases column spacing.
\resizebox{\textwidth}{!}{
% This right-aligns numbers in column, but centers them under column title.
\begin{tabular}{cccc}
   \hline
   \textbf{Variable} & \textbf{Description} & \textbf{$ee$} & \textbf{$\mu\mu$} \\
   \hline
    bTagDisc & b-tag discriminator of the leading b-tagged jet & $\checkmark$ & $\checkmark$ \\
    fourthJetPt & \pt of the fourth jet & $\checkmark$ & $\checkmark$ \\
    jetHt & Total \HT of every jet & $X$ & $\checkmark$ \\
    jetMass & Total mass of every jet & $\checkmark$ & $\checkmark$ \\
    jjDelR & $\Delta R$ between the leading jets & $\checkmark$ & $\checkmark$ \\
    leadJetEta & $\eta$ of the leading jet & $\checkmark$ & $\checkmark$ \\
    leadJetPt & \pt of the leading jet & $\checkmark$ & $\checkmark$ \\
    met & \met & $\checkmark$ & $\checkmark$ \\
    secJetPt & \pt of the second jet & $\checkmark$ & $\checkmark$ \\
    thirdJetPt & \pt of the third jet & $\checkmark$ & $\checkmark$ \\
    topMass & $m_{top}$ & $\checkmark$ & $\checkmark$ \\
    totHtOverPt & Total \HT divided by total \pt & $\checkmark$ & $\checkmark$ \\
    wPairMass & $m_{W}$ & $\checkmark$ & $\checkmark$ \\
    wQuark2Eta & $\eta$ of the second W boson candidate jet & $X$ & $\checkmark$ \\
    wwdelR & $\Delta R$ between the W boson candidate jets & $\checkmark$ & $X$ \\
    zEta & $\eta$ of the Z boson & $X$$ & $\checkmark$ \\
    zHt & \HT of the Z boson & $\checkmark$ & $\checkmark$ \\
    zMass & $m_{Z}$ & $\checkmark$ & $\checkmark$ \\
    zTopDelR & $\Delta R$ between the Z boson and top quark & $X$ & $\checkmark$ \\
    zjminR & Minimum $\Delta R$ between the Z boson and a jet & $\checkmark$ & $\checkmark$ \\
    zlb1DelR & $\Delta R$ between the Z boson and leading b-tagged jet & $\checkmark$ & $X$ \\
   \hline
 \end{tabular}}
\end{table}

\editComment{LOTS of PLOTS of the input variable distributions}

\subsection{BDT Training and Output}
Each sample of events for each process considered is split into a training and testing sample.


\chapter{Systematic Uncertainties}\label{chapter:systematics}
%%% Intro
In order to make a meaningful and robust measurement, it is vital to understand the and control the sources of systematic uncertainties associated with the physics analysis.
This is particularly important when searching for tZq given that whilst the event selections described in Chapter~\ref{} provide for large statistics regions, 

the process has a comparatively small cross section compared to the considerably larger backgrounds present, resulting in the statistical and systematic uncertainties to be of a comparable scale.


%%% Sources
These sources of uncertainty either originate from experimental or theoretical uncertainties and typically influence the result in one of two ways:
\begin{itemize}
\item \textbf{Rate or normalisation uncertainties} impact the number of events present and thus influence the  normalisation of the distributions considered.
\item \textbf{Shape or scale factor uncertainties} impact the shape of the distributions as they involve the scaling of individual events as a function of their kinematics in order to correct inconsistencies between simulation and data.
\end{itemize}

These uncertainties, as well as the statistical uncertainties arising from the size of the simulated samples available, are treated as nuisance parameters in the statistical fit model which is discussed in~\ref{chapter:results}.

\section{Experimental Uncertainties}
\subsection{Jet Energy Corrections}
The Jet Energy Corrections group also provides the uncertainties associated with the JES and JER they determine, discussed in Chapters~\ref{subsubsec:JECs} and~\ref{subsec:jesjer}, are determined by the Jet Energy Corrections group~\cite{Khachatryan:2016kdb}. 

The impact that the JES has on the jet kinematics is evaluated by varying the corrective JES up and down by a standard deviation.
The uncertainty associated with the JER smearing is accounted for by varying the smearing factor up and down by the associated statistical uncertainty.
Recently the uncertainties associated with the JER have been updated during the reprocessing of the 2016 dataset to include the systematic uncertainties in addition to the statistical uncertainties.
At the time of writing this thesis, these reprocessed samples and the impact of the revised total JER uncertainties has not been propagated through the analysis.

The impact of the uncertainties associated with both the JES and JER on the \MET are accounted for by propagating the JEC uncertainties through to the \MET and evaluating the impact they have.

\subsection{Pileup Reweighting}
The uncertainty associated with the primary vertex distributions used in the \PU reweighting is determined by varying the expected minimum bias cross section used in simulation $\pm X%$ in order to ascertain the impact of greater or lesser amounts of \PU on the analysis.

\subsection{Parton Density Functions}\label{subsec:pdfSysts}
%%Discussion of what PDFs are, is given in an earlier chapter 
The impact of the PDF uncertainties are evaluated according to the PDF4LHC recommendations~\cite{Butterworth:2015oua}, where they are estimated as the standard deviation of the weights of the nominal and the variations of the PDF set.

For almost all of the MC samples considered, this is achieved by considering the nonimal event weight and one hundred alternative PDF weights which are stored as per-event weights in the LHE 
event header for almost all of the MC samples considered.

The single top tW-channel samples are the exception to this as at the time of their generation it was not possible to generate per-event weights to account for the PDF variations for this process.
Therefore, the LHAPDF (Les Houches Accord Parton Distribution Function) library is used to access both the nominal PDF weight and 50 eigenvalues from the NNPDF3.0 set to provide one hundred alternative event weights to be evaluated.

\subsection{b-tagging Uncertainties}
The uncertainties associated with the b-tagging scale factors described in Chapter~\ref{subsec:btagEff} are obtained by varying their value by $\pm 1\sigma$, as calculated by the BTV POG.

\subsection{Non-prompt Lepton Contributions}
As this data-driven estimate of the instrumental backgrounds should have no dependence on either the lepton flavour or selection cuts, the variation of the ratio of opposite-sign over same-sign events as a function of the lepton flavour and the cut level was considered to be well accounted for by a 30\% rate uncertainty.

\subsection{Luminosity Uncertainties}
CMS uses the pixel detector, DTs, HF, the Fast Beam Conditions Monitor and Pixel Luminosity Telescope to monitor and measure the instantaneous and integrated luminosity.
During Run 2, the primary offline luminosity measurements made by the CMS Luminosity Group used the pixel detector using the Pixel Cluster Counting (PCC) method due its stability over time for up an average \PU of 150 and the high precision results obtained with it during Run I.
The PCC algorithm is able to achieve such a precision by measuring the instantaneous luminosity through the number of pixels present. 
This is possible as the probability of pixel hit belonging to multiple tracks is very small due to the very low occupancy of the detector, inferring that the number of pixel hits are linearly proportional to the number of interactions during a bunch crossing~\cite{CMS:2017_lumi}.

Using Van der Meer (VdM) scans during dedicated LHC runs to calibrate the absolute luminosity scale calibrations of the detectors~\cite{vanderMeer:1968zz}.
The overall uncertainty in the integrated luminosity collected by CMS in 2016 was estimated to be 2.5\%~\cite{CMS:2017_lumi}.

%The MC events produced are weighted by a scale factor in order to correctly normalise them with respect to the data they are compared against.
%This normalisation scale factor is given by:
%\begin{equation}
%SF_{dataset} = \frac{\pazocal{L} \sigma}{N_{MC}^{Events}}
%\end{equation}
%where $\pazocal{L}$ is the amount of total integrated luminosity considered in the data used, $\sigma$ the cross section of the MC sample considered and $N_{MC}^{Events}$ is number of simulated events considered for the process.

\subsection{Lepton Efficiencies}
The uncertainties associated with the lepton identification, isolation and reconstruction efficiency scale factors discussed in Chapter~\ref{subsec:leptonRecoSFs} are varied +/- 1 sigma.

%The uncertainty associated with the lepton trigger efficiencies are calculated using 
%The conservative method of Clopper-Pearson intervals~\cite{Cousins:2009kz} was used to determine the uncertainities associated with the trigger scale factors used.

%The potential correlations between the cross triggers and lepton triggers used are one source of systematic uncertainty.
%If both triggers are independent, then the efficiency of fulfilling both trigger selections can be expressed as:
%\begin{equation}
%\epsilon_{X + lepton triggers} = \epsilon_{X triggers} \times \epsilon_{lepton triggers}
%\label{eq:triggerCorrelation}
%\end{equation}
%
%and if both trigger selections are uncorrelated, then the ratio of the left and right hand sides ($\alpha$) of Equation~\ref{eq:triggerCorrelation} would be 1.
%Table~\ref{tab:triggerAlpha} shows that the values of $\alpha$ determined for each channel only differ slightly from 1.
%\begin{table}[htbp]
%\topcaption {
%The values of $\alpha$, expressing the strength of correlation between the lepton and cross triggers used to determine the trigger scale factors, for each channel.
%}
%\label{tab:triggerCorrelation}
%  \centering
%  \resizebox{\textwidth}{!}{
%% This right-aligns numbers in column, but centers them under column title.
% \begin{tabular}{cc}
%   \hline
%   \textbf{Channel} & \textbf{$\alpha$}   \\
%   \hline   
%   ee & 1.0 \\
%   $\mumu$ & 1.0  \\
%   e$\mu$ & 1.0  \\
%   \hline
% \end{tabular}}
%\end{table}
%
%\editComment{Update values of alpha}

\section{Theoretical Uncertainties}\label{sec:theorySysts}

\subsection{Factorisation and renormalisation scales}
The factorisation and renormalisation scales ($\mu_{f}$,$\mu_{s}$) used at the Matrix Element and Parton Shower levels are parametrised as functions of $Q^{2}$.
In order to consider the impact of the uncertainty associated with the choice of scales used, $Q^{2}$ is varied up and down by factors of 2 and 0.5 respectively.

For the majority of the MC samples considered, the variations in $\mu_{f}$ and $\mu_{s}$ are stored in the LHE event header as per-event weights.
These weights are produced for where one scale is fixed as the other is varied or both are varied simultaneously.
The event weights for the simultaneously varied scales were used to reweighting each event in order to evaluate the impact of the $\mu_{f}$ and $\mu_{s}$ uncertainties.

In contrast to the ME level, the impact of the PS shower scale uncertainties was evaluated through the use of dedicated samples where the PS scale had been varied up and down.
These centrally produced samples are listed in Table~\ref{tab:theorySampleList} as the ``scale up'' and ``scale down'' samples.
In the case of \ttbar however, these samples are listed as ISR (initial-state radiation) and FSR (final-state radiation), as it includes the variations in the gluon emissions of the incoming and outgoing partons.

As mentioned above in Chapter~\ref{subsec:pdfSysts}, it was not possible for the single top tW-channel MC samples to be produced with per-event weights to account for the matrix element factorisation and renormalisation scales.
Dedicated samples for this process, listed in Table~\ref{tab:theorySampleList}, where the matrix element and parton shower scales are varied are used to evaluate these systematic uncertainties.

\subsection{Parton Shower Matching Thresholds}
As discussed in Chapter~\ref{subsec:eventGenerators}, all of the MC samples considered use model the hard scattering process through a dedicated Matrix Element generator, with PYTHIA 8 being used to perform the subsequent PS and hadronisation.

The uncertainty associated with the choice of the matching threshold used is evaluated by using the dedicated matching samples.
Such samples have been generated for the \ttbar and single top t-channel backgrounds, listed in Table~\ref{tab:theorySampleList}, where the model's matching threshold parameter \emph{hdamp} is varied up and down by one standard deviation~\cite{CMS:2016kle}.

\section{Impact of the Uncertainties}
The effect of each of the systematics considered on the event rate, in percentage, are shown in Table~\ref{tab:systImpact}.
These rates, whilst providing a useful insight into which of the systematics are the most important, do not show how the shape of each fitted variable and the MVA discriminant is influenced by each uncertainty.
\editComment{Make some comment on most important/impactful systematics and how better understanding them would improve the result}

\begin{table}[!htbp]
\begin{center}
\linespread{2}
\resizebox{\textwidth}{!}{\begin{tabular}{|l|c|c|c|c|}
\hline
Systematic      &  tZq                  & DY                   & \ttbar{}                  & Other         \\
($ee$ / $\mu\mu$) & (\%)  & (\%)  & (\%)  & (\%)  \\
\hline
Trigger             &  $_{-4.23\%}^{+4.24\%}$ /  $_{-0.21\%}^{+6.07\%}$   & $_{-4.72\%}^{+4.07\%}$ / $_{-0.32\%}^{+6.37\%}$  & $_{-5.08\%}^{+4.41\%}$ / $_{-0.55\%}^{+5.54\%}$ & $_{-4.72\%}^{+4.85\%}$ / $_{-4.47\%}^{+5.97\%}$  \\
JER             &  $_{-5.27\%}^{+6.02\%}$ /  $_{-6.11\%}^{+5.39\%}$   & $_{-11.81\%}^{+16.54\%}$ / $_{-14.18\%}^{+16.71\%}$  & $_{-7.98\%}^{+7.84\%}$ / $_{-6.13\%}^{+8.24\%}$  & $_{--1.96\%}^{+2.11\%}$ / $_{-1.62\%}^{+1.82\%}$  \\
JES             &  $_{-0.04\%}^{+0.19\%}$ /  $_{-0.13\%}^{+0.13\%}$   & $_{-0.55\%}^{+0.29\%}$ / $_{-0.17\%}^{+0.13\%}$  & $_{-1.30\%}^{+0.02\%}$ / $_{-0.20\%}^{+0.20\%}$  & $_{-0.0.01\%}^{+0.11\%}$ / $_{-0.14\%}^{+0.18\%}$  \\
Pileup             &  $_{-0.42\%}^{+0.43\%}$ /  $_{-0.17\%}^{+0.43\%}$   & $_{-2.35\%}^{+2.26\%}$ / $_{-2.57\%}^{+1.75\%}$  & $_{-1.52\%}^{+0.52\%}$ / $_{-0.09\%}^{+1.35\%}$  & $_{-0.86\%}^{+0.38\%}$ / $_{-0.15\%}^{+0.26\%}$  \\
bTag             &  $_{-2.78\%}^{+3.38\%}$ /  $_{-3.38\%}^{+2.99\%}$   & $_{-5.30\%}^{+5.11\%}$ / $_{-5.02\%}^{+5.12\%}$  & $_{-2.89\%}^{+3.02\%}$ / $_{-3.12\%}^{+3.77\%}$  & $_{-3.43\%}^{+3.25\%}$ / $_{-3.24\%}^{+3.00\%}$  \\    
PDF             &  $_{-9.98\%}^{+13.22\%}$ /  $_{-9.24\%}^{+11.94\%}$   & $_{-1.56\%}^{+1.73\%}$ / $_{-2.95\%}^{+2.16\%}$  & $_{-2.99\%}^{+1.85\%}$ / $_{-2.95\%}^{+2.16\%}$  & $_{-8.56\%}^{+9.95\%}$ / $_{-8.51\%}^{+9.40\%}$  \\
$Q^{2}$Scaling             &  $_{-2.82\%}^{+1.36\%}$ /  $_{-3.06\%}^{+1.33\%}$   & $_{-15.00\%}^{+2.92\%}$ / $_{-14.64\%}^{+2.05\%}$  & $_{-11.38\%}^{-1.38\%}$ / $_{-11.40\%}^{+0.0\%}$  & $_{-5.01\%}^{+1.37\%}$ / $_{-5.07\%}^{+1.8\%}$  \\
\hline
\end{tabular}
}
\caption{Rate impact of systematics on MC templates}\label{tab:systImpact}
\end{center}
\end{table}
